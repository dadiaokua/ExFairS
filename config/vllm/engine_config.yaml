# vLLM Engine Configuration
# This file contains all vLLM engine parameters

# Model Configuration
model_path: "/Users/myrick/modelHub/hub/models--Qwen--Qwen3-8B"
tokenizer_path: "/Users/myrick/modelHub/hub/Qwen3-8B"
request_model_name: "Qwen2.5-32B"

# Parallelism Configuration
tensor_parallel_size: 8
pipeline_parallel_size: 1

# Memory Configuration
gpu_memory_utilization: 0.9
swap_space: 0  # GB

# Model Limits
max_model_len: 8124
max_num_seqs: 128
max_num_batched_tokens: 65536

# Device Configuration
device: "cuda"
dtype: "float16"
quantization: "None"

# Features
trust_remote_code: true
enable_chunked_prefill: false
disable_log_stats: true
enable_prefix_caching: false

# Scheduling
scheduling_policy: "priority"

# Server Configuration
start_engine: true
vllm_url: "http://222.201.144.119:8000/v1"
api_key: "None"

# Tunnel Configuration
use_tunnel: 0
local_port: "8000"
remote_port: "10085"

