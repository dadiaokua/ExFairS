#!/usr/bin/env python3
"""
ä»»åŠ¡ç®¡ç†æ¨¡å—
å¤„ç†åŸºå‡†æµ‹è¯•ä»»åŠ¡çš„è®¾ç½®ã€åˆ›å»ºå’Œç®¡ç†
"""

import asyncio
import json
import logging
import sys
import os
import random
from transformers import AutoTokenizer
from argument_parser import safe_float_conversion

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥åŸºå‡†æµ‹è¯•ç›¸å…³æ¨¡å—
from BenchmarkClient.BenchmarkClient import BenchmarkClient
from util.BaseUtil import initialize_clients
from BenchmarkMonitor.BenchmarkMonitor import ExperimentMonitor
from config.Config import GLOBAL_CONFIG

# å°è¯•å¯¼å…¥é˜Ÿåˆ—ç®¡ç†å™¨
try:
    from RequestQueueManager.RequestQueueManager import RequestQueueManager, QueueStrategy
    queue_manager_available = True
except ImportError:
    queue_manager_available = False

logger = logging.getLogger(__name__)


async def setup_benchmark_tasks(args, all_results, request_queue, logger):
    """Setup and create benchmark tasks"""
    
    if not queue_manager_available:
        logger.warning("RequestQueueManager not available, queue experiments will be skipped")
    
    tasks = []
    clients = []

    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer, trust_remote_code=True)

    # åŠ è½½åŸå§‹promptæ•°æ®é›†
    logger.info("ğŸ“‚ åŠ è½½promptæ•°æ®é›†...")
    
    # ä½¿ç”¨åŸå§‹promptæ•°æ®é›†ï¼ˆå›ºå®šè¾“å‡ºé•¿åº¦ï¼‰
    short_prompts_file = "prompt_hub/short_prompts.json"
    long_prompts_file = "prompt_hub/long_prompts.json"
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(short_prompts_file) or not os.path.exists(long_prompts_file):
        logger.error(f"âŒ Promptæ•°æ®é›†ä¸å­˜åœ¨")
        logger.info(f"   éœ€è¦çš„æ–‡ä»¶:")
        logger.info(f"   - {short_prompts_file}")
        logger.info(f"   - {long_prompts_file}")
        logger.info(f"   è¯·ç¡®ä¿prompt_hubç›®å½•ä¸‹æœ‰è¿™ä¸¤ä¸ªæ–‡ä»¶")
        raise FileNotFoundError(f"Required prompt files not found: {short_prompts_file}, {long_prompts_file}")
    
    # åŠ è½½promptæ•°æ®é›†
    with open(short_prompts_file, "r", encoding="utf-8") as f:
        short_formatted_json = json.load(f)

    with open(long_prompts_file, "r", encoding="utf-8") as f:
        long_formatted_json = json.load(f)

    mix_formatted_json = short_formatted_json + long_formatted_json
    
    logger.info(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
    logger.info(f"   çŸ­prompt: {len(short_formatted_json)} æ¡")
    logger.info(f"   é•¿prompt: {len(long_formatted_json)} æ¡")
    logger.info(f"   æ€»è®¡: {len(mix_formatted_json)} æ¡")
    
    random.shuffle(mix_formatted_json)

    openAI_client = initialize_clients(args.local_port)

    # åˆ›å»ºå…±äº«çš„é˜Ÿåˆ—ç®¡ç†å™¨ï¼ˆå¦‚æœä½¿ç”¨é˜Ÿåˆ—å®éªŒï¼‰
    queue_manager = None
    queue_manager_task = None
    if args.exp.startswith("QUEUE_") and queue_manager_available:
        # æ ¹æ®å®éªŒç±»å‹é€‰æ‹©é˜Ÿåˆ—ç­–ç•¥
        strategy_map = {
            "QUEUE_FIFO": QueueStrategy.FIFO,
            "QUEUE_FCFS": QueueStrategy.FIFO,
            "QUEUE_LFS": QueueStrategy.PRIORITY,
            "QUEUE_ExFairS": QueueStrategy.PRIORITY,  # ExFairS uses priority-based scheduling
            "QUEUE_ROUND_ROBIN": QueueStrategy.ROUND_ROBIN,
            "QUEUE_VTC": QueueStrategy.VTC,
            "QUEUE_MINQUE": QueueStrategy.PRIORITY,
            "QUEUE_Justitia": QueueStrategy.JUSTITIA,  # Justitia virtual time scheduling
            "QUEUE_SLOGreedy": QueueStrategy.SLO_GREEDY  # SLO violation rate greedy scheduling
        }

        strategy = strategy_map.get(args.exp, QueueStrategy.FIFO)
        logger.info(f"Using queue strategy: {strategy.value}")
        queue_manager = RequestQueueManager(strategy=strategy, max_queue_size=20000)
        
        # é…ç½®é˜Ÿåˆ—ç®¡ç†å™¨
        if strategy == QueueStrategy.PRIORITY:
            # é…ç½®éƒ¨åˆ†ä¼˜å…ˆçº§å‚æ•°
            queue_manager.configure_partial_priority(
                insert_multiplier=2, 
                max_positions=50,
                delay_enabled=False,  # ç¦ç”¨å»¶è¿Ÿ
                max_delay=0  # è®¾ç½®æœ€å¤§å»¶è¿Ÿä¸º0
            )
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œä½œä¸ºå¤‡ç”¨ï¼‰
        if openAI_client:
            queue_manager.set_openai_client(openAI_client)
            logger.info(f"âœ“ OpenAI client set as fallback: {len(openAI_client)} clients")
        else:
            logger.info("No OpenAI client provided (will use vLLM engine directly)")
        
        # éªŒè¯å¤„ç†èƒ½åŠ›
        vllm_engine = GLOBAL_CONFIG.get('vllm_engine')
        
        if vllm_engine is not None:
            logger.info("âœ“ vLLM engine is available for primary request processing")
        elif queue_manager.openai_client is not None:
            logger.info("âœ“ Will use OpenAI client for request processing (fallback mode)")
        else:
            logger.error("CRITICAL: Neither vLLM engine nor OpenAI client is available")
            raise RuntimeError("No request processing method available")
        
        # åŠ¨æ€è®¡ç®— worker æ•°é‡ï¼šåŸºäºæ€»å®¢æˆ·ç«¯æ•°å’Œæ€» QPM
        total_clients = args.short_clients + args.long_clients + args.mix_clients
        
        # è®¡ç®—æ€» QPM
        total_qpm = 0
        for i in range(args.short_clients):
            qpm = safe_float_conversion(args.short_qpm[0] if len(args.short_qpm) == 1 else args.short_qpm[i])
            total_qpm += qpm
        for i in range(args.long_clients):
            qpm = safe_float_conversion(args.long_qpm[0] if len(args.long_qpm) == 1 else args.long_qpm[i])
            total_qpm += qpm
        for i in range(args.mix_clients):
            qpm = safe_float_conversion(args.mix_qpm[0] if len(args.mix_qpm) == 1 else args.mix_qpm[i])
            total_qpm += qpm
        
        # è®¡ç®— worker æ•°é‡ï¼š
        # - åŸºç¡€ï¼šæ¯ä¸ªå®¢æˆ·ç«¯è‡³å°‘ 1 ä¸ª worker
        # - æŒ‰ QPM å¢åŠ ï¼šæ¯ 10 QPM å¢åŠ  1 ä¸ª worker
        # - æœ€å° 5ï¼Œæœ€å¤§ 50
        base_workers = total_clients
        qpm_workers = int(total_qpm / 10)
        num_workers = max(5, min(50, base_workers + qpm_workers))
        
        logger.info(f"ğŸ“Š Dynamic worker calculation: {total_clients} clients, {total_qpm:.0f} total QPM â†’ {num_workers} workers")
        
        # å¯åŠ¨é˜Ÿåˆ—ç®¡ç†å™¨ï¼ˆåœ¨åå°è¿è¡Œï¼‰
        queue_manager_task = asyncio.create_task(queue_manager.start_processing(num_workers=num_workers))
        logger.info(f"Created queue manager with strategy: {strategy.value}, workers: {num_workers}")
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿é˜Ÿåˆ—ç®¡ç†å™¨æ­£å¸¸å¯åŠ¨
        await asyncio.sleep(2.0)
        
        # æ£€æŸ¥é˜Ÿåˆ—ç®¡ç†å™¨çŠ¶æ€
        if queue_manager.is_running and queue_manager.workers_running:
            logger.info(f"âœ“ Queue manager started successfully: is_running={queue_manager.is_running}, workers_running={queue_manager.workers_running}")
        else:
            logger.error(f"âŒ Queue manager failed to start properly. is_running: {queue_manager.is_running}, workers_running: {queue_manager.workers_running}")
            # å³ä½¿å¯åŠ¨å¤±è´¥ä¹Ÿç»§ç»­ï¼Œå¯èƒ½åœ¨åç»­ä½¿ç”¨ä¸­æ¢å¤

    # Create short request clients
    for index in range(args.short_clients):
        qpm_value = safe_float_conversion(args.short_qpm[0] if len(args.short_qpm) == 1 else args.short_qpm[index])
        slo_value = safe_float_conversion(
            args.short_clients_slo[0] if len(args.short_clients_slo) == 1 else args.short_clients_slo[index], 10)

        client = BenchmarkClient(
            client_type='short',
            client_index=index,
            qpm=qpm_value,
            port=args.local_port,
            api_key=args.api_key,
            distribution=args.distribution,
            request_timeout=args.request_timeout,
            concurrency=args.concurrency,
            round_time=args.round_time,
            sleep=args.sleep,
            result_queue=all_results,
            use_time_data=args.use_time_data,
            formatted_json=short_formatted_json,
            OpenAI_client=openAI_client,
            tokenizer=tokenizer,
            time_data=None,
            round=args.round,
            exp_type=args.exp,
            qpm_ratio=args.short_client_qpm_ratio,
            latency_slo=int(slo_value),
            queue_manager=queue_manager  # ä¼ é€’é˜Ÿåˆ—ç®¡ç†å™¨
        )
        clients.append(client)
        tasks.append(client.start())

    # Create long request clients
    for index in range(args.long_clients):
        qpm_value = safe_float_conversion(args.long_qpm[0] if len(args.long_qpm) == 1 else args.long_qpm[index])
        slo_value = safe_float_conversion(
            args.long_clients_slo[0] if len(args.long_clients_slo) == 1 else args.long_clients_slo[index], 10)

        client = BenchmarkClient(
            client_type='long',
            client_index=index,
            qpm=qpm_value,
            port=args.local_port,
            api_key=args.api_key,
            distribution=args.distribution,
            request_timeout=args.request_timeout,
            concurrency=args.concurrency,
            round_time=args.round_time,
            sleep=args.sleep,
            result_queue=all_results,
            use_time_data=args.use_time_data,
            formatted_json=long_formatted_json,
            OpenAI_client=openAI_client,
            tokenizer=tokenizer,
            time_data=None,
            round=args.round,
            exp_type=args.exp,
            qpm_ratio=args.long_client_qpm_ratio,
            latency_slo=int(slo_value),
            queue_manager=queue_manager  # ä¼ é€’é˜Ÿåˆ—ç®¡ç†å™¨
        )
        clients.append(client)
        tasks.append(client.start())
        
    for index in range(args.mix_clients):
        qpm_value = safe_float_conversion(args.mix_qpm[0] if len(args.mix_qpm) == 1 else args.mix_qpm[index])
        slo_value = safe_float_conversion(
            args.mix_clients_slo[0] if len(args.mix_clients_slo) == 1 else args.mix_clients_slo[index], 10)
        
        client = BenchmarkClient(
            client_type='mix',
            client_index=index,
            qpm=qpm_value,
            port=args.local_port,
            api_key=args.api_key,
            distribution=args.distribution,
            request_timeout=args.request_timeout,
            concurrency=args.concurrency,
            round_time=args.round_time,
            sleep=args.sleep,
            result_queue=all_results,
            use_time_data=args.use_time_data,
            formatted_json=mix_formatted_json,
            OpenAI_client=openAI_client,
            tokenizer=tokenizer,
            time_data=None,
            round=args.round,
            exp_type=args.exp,
            qpm_ratio=args.mix_client_qpm_ratio,
            latency_slo=int(slo_value),
            queue_manager=queue_manager  # ä¼ é€’é˜Ÿåˆ—ç®¡ç†å™¨
        )
        clients.append(client)
        tasks.append(client.start())

    # åˆ›å»ºç›‘æ§å™¨å®ä¾‹
    monitor = ExperimentMonitor(clients, all_results, args.short_clients + args.long_clients + args.mix_clients, args.exp, request_queue,
                                args.use_tunnel)

    # åˆ›å»ºç›‘æ§ä»»åŠ¡
    monitor_task = asyncio.create_task(monitor())
    tasks.insert(0, monitor_task)

    # å¦‚æœä½¿ç”¨é˜Ÿåˆ—ç®¡ç†å™¨ï¼Œå¯åŠ¨é˜Ÿåˆ—å¤„ç†ï¼ˆä½†ä¸åŠ å…¥tasksï¼Œè®©å®ƒåœ¨åå°è¿è¡Œï¼‰
    if queue_manager:
        # é˜Ÿåˆ—ç®¡ç†å™¨å·²ç»åœ¨setup_benchmark_tasksä¸­å¯åŠ¨äº†ï¼Œè¿™é‡Œåªéœ€è¦è®°å½•ä¸€ä¸‹
        logger.info(f"Queue manager is running in background with strategy: {queue_manager.strategy.value}")

    return tasks, monitor_task, clients, queue_manager


async def run_benchmark_tasks(tasks, logger):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•ä»»åŠ¡"""
    benchmark_timeout = GLOBAL_CONFIG.get('exp_time', 36000)
    
    try:
        await asyncio.wait_for(asyncio.gather(*tasks[1:]), timeout=benchmark_timeout)
    except asyncio.TimeoutError:
        logger.error(f"Tasks did not complete within {benchmark_timeout} seconds, cancelling...")
        for task in tasks:
            if not task.done():
                task.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)
    except Exception as e:
        logger.error(f"An error occurred: {e}")
        for task in tasks:
            if not task.done():
                task.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)


async def cancel_monitor_task(monitor_task, logger):
    """å–æ¶ˆç›‘æ§ä»»åŠ¡"""
    monitor_task.cancel()
    try:
        await monitor_task
    except asyncio.CancelledError:
        logger.info("Monitor task cancelled.") 