#!/usr/bin/env python3
"""
æµ‹è¯•vLLMå¼•æ“è°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§

æµ‹è¯•å†…å®¹ï¼š
1. å¯åŠ¨çœŸå®vLLMå¼•æ“
2. ç›´æ¥å‘vLLMå¼•æ“æäº¤å¤šä¸ªè¯·æ±‚
3. ç›‘æ§vLLMè°ƒåº¦å™¨ä¸­çš„è¯·æ±‚æ•°é‡å˜åŒ–ï¼ˆç­‰å¾…é˜Ÿåˆ—ã€è¿è¡Œé˜Ÿåˆ—ã€äº¤æ¢é˜Ÿåˆ—ï¼‰
4. éªŒè¯èƒ½å¦æœ‰æ•ˆè·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
"""

import asyncio
import time
import logging
import uuid
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vllm_engine_helper import VLLMEngineManager
from vllm import SamplingParams

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_sampling_params(max_tokens=256):
    """åˆ›å»ºé‡‡æ ·å‚æ•°"""
    return SamplingParams(
        temperature=0.8,
        top_p=0.95,
        max_tokens=max_tokens,
        stop=None
    )


async def collect_generation_output(engine, prompt, sampling_params, request_id):
    """æ”¶é›†ç”Ÿæˆè¾“å‡º"""
    try:
        start_time = time.time()
        logger.info(f"å¼€å§‹å¤„ç†è¯·æ±‚: {request_id}")
        
        # æäº¤è¯·æ±‚åˆ°vLLMå¼•æ“
        results = []
        async for request_output in engine.generate(prompt, sampling_params, request_id):
            results.append(request_output)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        if results:
            final_output = results[-1]
            output_text = final_output.outputs[0].text if final_output.outputs else ""
            output_tokens = len(final_output.outputs[0].token_ids) if final_output.outputs else 0
            
            logger.info(f"å®Œæˆè¯·æ±‚: {request_id}, è€—æ—¶: {total_time:.3f}s, è¾“å‡ºtokens: {output_tokens}")
            return {
                'request_id': request_id,
                'status': 'completed',
                'total_time': total_time,
                'output_tokens': output_tokens
            }
        else:
            logger.warning(f"è¯·æ±‚ {request_id} æ²¡æœ‰äº§ç”Ÿè¾“å‡º")
            return None
            
    except Exception as e:
        logger.error(f"è¯·æ±‚ {request_id} å¤„ç†å¤±è´¥: {e}")
        return None


async def monitor_vllm_scheduler(engine, duration=20, interval=0.5):
    """ç›‘æ§vLLMè°ƒåº¦å™¨çŠ¶æ€"""
    logger.info(f"å¼€å§‹ç›‘æ§vLLMè°ƒåº¦å™¨çŠ¶æ€ï¼ŒæŒç»­ {duration} ç§’...")
    
    start_time = time.time()
    max_waiting = 0
    max_running = 0
    max_swapped = 0
    total_samples = 0
    monitoring_data = []
    
    while time.time() - start_time < duration:
        try:
            if hasattr(engine, 'engine') and hasattr(engine.engine, 'scheduler'):
                scheduler = engine.engine.scheduler
                waiting_count = len(scheduler.waiting) if hasattr(scheduler, 'waiting') else 0
                running_count = len(scheduler.running) if hasattr(scheduler, 'running') else 0
                swapped_count = len(scheduler.swapped) if hasattr(scheduler, 'swapped') else 0
                
                max_waiting = max(max_waiting, waiting_count)
                max_running = max(max_running, running_count)
                max_swapped = max(max_swapped, swapped_count)
                total_samples += 1
                
                elapsed = time.time() - start_time
                
                monitoring_data.append({
                    'elapsed': elapsed,
                    'waiting': waiting_count,
                    'running': running_count,
                    'swapped': swapped_count
                })
                
                logger.info(f"[{elapsed:.1f}s] vLLMè°ƒåº¦å™¨çŠ¶æ€ - ç­‰å¾…: {waiting_count}, è¿è¡Œ: {running_count}, äº¤æ¢: {swapped_count}")
                
                # å¦‚æœæ‰€æœ‰é˜Ÿåˆ—éƒ½ä¸ºç©ºä¸”å·²ç»è¿è¡Œäº†è‡³å°‘5ç§’ï¼Œå¯ä»¥æå‰ç»“æŸ
                if waiting_count == 0 and running_count == 0 and swapped_count == 0 and elapsed > 5:
                    logger.info("æ‰€æœ‰é˜Ÿåˆ—ä¸ºç©ºï¼Œç›‘æ§ç»“æŸ")
                    break
                    
            else:
                logger.warning("æ— æ³•è®¿é—®vLLMè°ƒåº¦å™¨")
                
        except Exception as e:
            logger.debug(f"ç›‘æ§è°ƒåº¦å™¨çŠ¶æ€æ—¶å‡ºé”™: {e}")
            
        await asyncio.sleep(interval)
    
    logger.info(f"ç›‘æ§å®Œæˆ - æœ€å¤§ç­‰å¾…: {max_waiting}, æœ€å¤§è¿è¡Œ: {max_running}, æœ€å¤§äº¤æ¢: {max_swapped}, æ€»é‡‡æ ·: {total_samples}")
    return {
        'max_waiting': max_waiting,
        'max_running': max_running, 
        'max_swapped': max_swapped,
        'total_samples': total_samples,
        'monitoring_data': monitoring_data
    }


async def test_vllm_scheduler_queue_monitoring():
    """æµ‹è¯•vLLMè°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•vLLMè°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§ ===")
    
    # å¯åŠ¨vLLMå¼•æ“
    engine_manager = VLLMEngineManager()
    try:
        logger.info("å¯åŠ¨vLLMå¼•æ“...")
        engine = await engine_manager.create_engine(
            model_path="/home/llm/model_hub/Llama-3.1-8B",
            max_num_seqs=12,  # å¢åŠ åˆ°12ä¸ªå¹¶å‘åºåˆ—ä»¥å¤„ç†æ›´å¤šè¯·æ±‚
            tensor_parallel_size=8,
            suppress_logs=True
        )
        
        logger.info("âœ“ vLLMå¼•æ“å¯åŠ¨æˆåŠŸ")
        
        # å‡†å¤‡æµ‹è¯•prompts
        prompts = [
            "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨é¢†åŸŸ",
            "æè¿°æ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
            "è§£é‡Šæœºå™¨å­¦ä¹ ç®—æ³•çš„åˆ†ç±»",
            "è®¨è®ºè®¡ç®—æœºè§†è§‰çš„å‘å±•ç°çŠ¶",
            "åˆ†æå¤§æ•°æ®å¤„ç†æŠ€æœ¯",
            "ä»‹ç»äº‘è®¡ç®—çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜",
            "æ¢è®¨åŒºå—é“¾æŠ€æœ¯çš„åº”ç”¨å‰æ™¯",
            "è¯´æ˜ç‰©è”ç½‘æŠ€æœ¯çš„æ ¸å¿ƒç‰¹ç‚¹",
            "é˜è¿°ç½‘ç»œå®‰å…¨çš„é‡è¦æ€§",
            "ä»‹ç»åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ¶æ„è®¾è®¡",
            "è®¨è®ºæ•°æ®åº“ä¼˜åŒ–ç­–ç•¥",
            "è§£é‡Šè½¯ä»¶å·¥ç¨‹çš„åŸºæœ¬åŸåˆ™",
            "åˆ†ææ“ä½œç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½",
            "æè¿°ç½‘ç»œåè®®çš„å·¥ä½œæœºåˆ¶",
            "æ¢è®¨ç§»åŠ¨åº”ç”¨å¼€å‘è¶‹åŠ¿",
            "ä»‹ç»DevOpså®è·µæ–¹æ³•",
            "è®¨è®ºå¾®æœåŠ¡æ¶æ„è®¾è®¡",
            "è§£é‡Šå®¹å™¨åŒ–æŠ€æœ¯ä¼˜åŠ¿",
            "åˆ†æå‰ç«¯å¼€å‘æ¡†æ¶é€‰æ‹©",
            "æè¿°åç«¯æœåŠ¡è®¾è®¡æ¨¡å¼",
            "æ¢è®¨APIè®¾è®¡æœ€ä½³å®è·µ",
            "ä»‹ç»æµ‹è¯•é©±åŠ¨å¼€å‘æ–¹æ³•",
            "è®¨è®ºä»£ç è´¨é‡ç®¡ç†",
            "è§£é‡Šæ€§èƒ½ä¼˜åŒ–ç­–ç•¥"
        ] * 2  # 50ä¸ªè¯·æ±‚
        
        sampling_params = create_sampling_params(max_tokens=150)
        
        logger.info(f"å‡†å¤‡æäº¤ {len(prompts)} ä¸ªè¯·æ±‚...")
        
        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        monitor_task = asyncio.create_task(monitor_vllm_scheduler(engine, duration=45, interval=0.5))  # å¢åŠ ç›‘æ§æ—¶é—´
        
        # å¿«é€Ÿè¿ç»­æäº¤æ‰€æœ‰è¯·æ±‚
        tasks = []
        submit_start = time.time()
        
        for i, prompt in enumerate(prompts):
            request_id = f"test_{i}_{uuid.uuid4().hex[:6]}"
            
            task = asyncio.create_task(
                collect_generation_output(engine, prompt, sampling_params, request_id)
            )
            tasks.append(task)
            
            logger.info(f"æäº¤è¯·æ±‚ {i+1}: {request_id}")
            await asyncio.sleep(0.1)  # å¿«é€Ÿæäº¤ï¼Œé—´éš”100ms
        
        submit_time = time.time() - submit_start
        logger.info(f"âœ“ æ‰€æœ‰è¯·æ±‚æäº¤å®Œæˆï¼Œè€—æ—¶: {submit_time:.3f}s")
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        logger.info("ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ...")
        start_wait = time.time()
        
        completed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        completion_time = time.time() - start_wait
        logger.info(f"âœ“ æ‰€æœ‰è¯·æ±‚å¤„ç†å®Œæˆï¼Œè€—æ—¶: {completion_time:.3f}s")
        
        # åœæ­¢ç›‘æ§
        monitor_task.cancel()
        try:
            monitor_stats = await monitor_task
        except asyncio.CancelledError:
            monitor_stats = {'max_waiting': 0, 'max_running': 0, 'max_swapped': 0, 'total_samples': 0}
        
        # åˆ†æç»“æœ
        successful_results = [r for r in completed_results if r is not None and not isinstance(r, Exception)]
        failed_results = [r for r in completed_results if r is None or isinstance(r, Exception)]
        
        logger.info("=== æµ‹è¯•ç»“æœåˆ†æ ===")
        logger.info(f"æ€»è¯·æ±‚æ•°: {len(prompts)}")
        logger.info(f"æˆåŠŸå®Œæˆ: {len(successful_results)}")
        logger.info(f"å¤±è´¥/å¼‚å¸¸: {len(failed_results)}")
        logger.info(f"è¯·æ±‚æäº¤æ—¶é—´: {submit_time:.3f}s")
        logger.info(f"è¯·æ±‚å®Œæˆæ—¶é—´: {completion_time:.3f}s")
        logger.info(f"vLLMæœ€å¤§ç­‰å¾…é˜Ÿåˆ—: {monitor_stats['max_waiting']}")
        logger.info(f"vLLMæœ€å¤§è¿è¡Œé˜Ÿåˆ—: {monitor_stats['max_running']}")
        logger.info(f"vLLMæœ€å¤§äº¤æ¢é˜Ÿåˆ—: {monitor_stats['max_swapped']}")
        logger.info(f"ç›‘æ§é‡‡æ ·æ¬¡æ•°: {monitor_stats['total_samples']}")
        
        if successful_results:
            avg_time = sum(r['total_time'] for r in successful_results) / len(successful_results)
            avg_tokens = sum(r['output_tokens'] for r in successful_results) / len(successful_results)
            logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.3f}s")
            logger.info(f"å¹³å‡è¾“å‡ºtokens: {avg_tokens:.1f}")
        
        # éªŒè¯ç›‘æ§æ•ˆæœ
        assert monitor_stats['total_samples'] > 10, "ç›‘æ§é‡‡æ ·æ¬¡æ•°åº”è¯¥è¶³å¤Ÿå¤š"
        assert monitor_stats['max_running'] > 0, "vLLMè¿è¡Œé˜Ÿåˆ—åº”è¯¥æœ‰è¯·æ±‚åœ¨å¤„ç†"
        assert len(successful_results) >= len(prompts) * 0.8, f"æˆåŠŸç‡åº”è¯¥è¾ƒé«˜: {len(successful_results)}/{len(prompts)}"
        
        # éªŒè¯æ˜¯å¦æˆåŠŸç›‘æ§åˆ°é˜Ÿåˆ—çŠ¶æ€å˜åŒ–
        if monitor_stats['max_waiting'] > 0:
            logger.info("âœ“ æˆåŠŸç›‘æ§åˆ°ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¯·æ±‚")
        else:
            logger.info("âš  æœªç›‘æ§åˆ°ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ï¼ˆå¯èƒ½è¯·æ±‚å¤„ç†å¤ªå¿«ï¼‰")
        
        logger.info("âœ“ vLLMè°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§æµ‹è¯•é€šè¿‡")
        
        return monitor_stats
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ¸…ç†èµ„æº
        await engine_manager.cleanup()


async def test_queue_capacity_limits():
    """æµ‹è¯•é˜Ÿåˆ—å®¹é‡é™åˆ¶"""
    logger.info("=== æµ‹è¯•é˜Ÿåˆ—å®¹é‡é™åˆ¶ ===")
    
    engine_manager = VLLMEngineManager()
    try:
        # ä½¿ç”¨è¾ƒå°çš„max_num_seqsæ¥è§‚å¯Ÿé˜Ÿåˆ—è¡Œä¸º
        engine = await engine_manager.create_engine(
            model_path="/home/llm/model_hub/Llama-3.1-8B",
            max_num_seqs=4,  # åªå…è®¸4ä¸ªå¹¶å‘åºåˆ—
            tensor_parallel_size=8,
            suppress_logs=True
        )
        
        logger.info("âœ“ vLLMå¼•æ“å¯åŠ¨æˆåŠŸ (max_num_seqs=4)")
        
        # åˆ›å»ºè¾ƒå¤šçš„è¯·æ±‚æ¥åˆ¶é€ é˜Ÿåˆ—å‹åŠ›
        prompts = [f"è¯·è¯¦ç»†å›ç­”é—®é¢˜{i}ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ä»å†å²å‘å±•ã€æŠ€æœ¯åŸç†ã€åº”ç”¨åœºæ™¯ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œåˆ†æã€‚" for i in range(20)]  # 20ä¸ªè¯·æ±‚
        
        sampling_params = create_sampling_params(max_tokens=100)
        
        # å¯åŠ¨ç›‘æ§
        monitor_task = asyncio.create_task(monitor_vllm_scheduler(engine, duration=35, interval=0.3))  # å¢åŠ ç›‘æ§æ—¶é—´
        
        # å¿«é€Ÿæäº¤æ‰€æœ‰è¯·æ±‚
        tasks = []
        for i, prompt in enumerate(prompts):
            request_id = f"capacity_test_{i}_{uuid.uuid4().hex[:4]}"
            
            task = asyncio.create_task(
                collect_generation_output(engine, prompt, sampling_params, request_id)
            )
            tasks.append(task)
            
            logger.info(f"æäº¤è¯·æ±‚ {i+1}: {request_id}")
            await asyncio.sleep(0.05)  # å¿«é€Ÿæäº¤
        
        logger.info("è§‚å¯Ÿé˜Ÿåˆ—çŠ¶æ€å˜åŒ–...")
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        completed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åœæ­¢ç›‘æ§
        monitor_task.cancel()
        try:
            monitor_stats = await monitor_task
        except asyncio.CancelledError:
            monitor_stats = {'max_waiting': 0, 'max_running': 0, 'max_swapped': 0}
        
        successful_count = len([r for r in completed_results if r is not None and not isinstance(r, Exception)])
        
        logger.info("=== é˜Ÿåˆ—å®¹é‡æµ‹è¯•ç»“æœ ===")
        logger.info(f"è¯·æ±‚æ€»æ•°: {len(prompts)}")
        logger.info(f"æˆåŠŸå®Œæˆ: {successful_count}")
        logger.info(f"æœ€å¤§ç­‰å¾…é˜Ÿåˆ—: {monitor_stats['max_waiting']}")
        logger.info(f"æœ€å¤§è¿è¡Œé˜Ÿåˆ—: {monitor_stats['max_running']}")
        logger.info(f"æœ€å¤§äº¤æ¢é˜Ÿåˆ—: {monitor_stats['max_swapped']}")
        
        # éªŒè¯é˜Ÿåˆ—å®¹é‡é™åˆ¶æ•ˆæœ
        assert monitor_stats['max_running'] <= 4, f"è¿è¡Œé˜Ÿåˆ—ä¸åº”è¶…è¿‡max_num_seqsé™åˆ¶: {monitor_stats['max_running']} > 4"
        assert monitor_stats['max_waiting'] > 0, "ç­‰å¾…é˜Ÿåˆ—åº”è¯¥æœ‰è¯·æ±‚å †ç§¯"
        assert successful_count >= len(prompts) * 0.8, "å¤§éƒ¨åˆ†è¯·æ±‚åº”è¯¥æˆåŠŸå®Œæˆ"
        
        logger.info("âœ“ é˜Ÿåˆ—å®¹é‡é™åˆ¶æµ‹è¯•é€šè¿‡")
        
        return monitor_stats
        
    finally:
        await engine_manager.cleanup()


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("å¼€å§‹è¿è¡ŒvLLMè°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§æµ‹è¯•å¥—ä»¶")
    
    try:
        # æµ‹è¯•åŸºæœ¬é˜Ÿåˆ—ç›‘æ§
        stats1 = await test_vllm_scheduler_queue_monitoring()
        
        # æµ‹è¯•é˜Ÿåˆ—å®¹é‡é™åˆ¶
        stats2 = await test_queue_capacity_limits()
        
        # ç»¼åˆåˆ†æ
        logger.info("=== ç»¼åˆæµ‹è¯•ç»“æœ ===")
        logger.info(f"åŸºæœ¬ç›‘æ§æµ‹è¯• - æœ€å¤§ç­‰å¾…: {stats1['max_waiting']}, æœ€å¤§è¿è¡Œ: {stats1['max_running']}")
        logger.info(f"å®¹é‡é™åˆ¶æµ‹è¯• - æœ€å¤§ç­‰å¾…: {stats2['max_waiting']}, æœ€å¤§è¿è¡Œ: {stats2['max_running']}")
        
        # éªŒè¯ç›‘æ§ç³»ç»Ÿçš„æœ‰æ•ˆæ€§
        total_waiting = stats1['max_waiting'] + stats2['max_waiting']
        total_running = stats1['max_running'] + stats2['max_running']
        
        assert total_waiting > 0, "ä¸¤æ¬¡æµ‹è¯•ä¸­åº”è¯¥è‡³å°‘è§‚å¯Ÿåˆ°ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰è¯·æ±‚"
        assert total_running > 0, "ä¸¤æ¬¡æµ‹è¯•ä¸­åº”è¯¥è‡³å°‘è§‚å¯Ÿåˆ°è¿è¡Œé˜Ÿåˆ—ä¸­æœ‰è¯·æ±‚"
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼vLLMè°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        logger.info("âœ“ å¯ä»¥æœ‰æ•ˆç›‘æ§ç­‰å¾…é˜Ÿåˆ—ã€è¿è¡Œé˜Ÿåˆ—å’Œäº¤æ¢é˜Ÿåˆ—çš„çŠ¶æ€")
        logger.info("âœ“ max_num_seqsé…ç½®ç”Ÿæ•ˆï¼Œèƒ½å¤Ÿé™åˆ¶å¹¶å‘å¤„ç†æ•°é‡")
        logger.info("âœ“ é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯å‡†ç¡®å¯é ")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    asyncio.run(run_all_tests()) 