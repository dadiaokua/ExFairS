#!/usr/bin/env python3
"""
æµ‹è¯•vLLMå¼•æ“è°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§ - å¸¦round timeé™åˆ¶

æµ‹è¯•å†…å®¹ï¼š
1. å¯åŠ¨çœŸå®vLLMå¼•æ“
2. ç›´æ¥å‘vLLMå¼•æ“æäº¤å¤šä¸ªè¯·æ±‚
3. ç›‘æ§vLLMè°ƒåº¦å™¨ä¸­çš„è¯·æ±‚æ•°é‡å˜åŒ–ï¼ˆç­‰å¾…é˜Ÿåˆ—ã€è¿è¡Œé˜Ÿåˆ—ã€äº¤æ¢é˜Ÿåˆ—ï¼‰
4. éªŒè¯round timeé™åˆ¶ä¸‹çš„å¼‚æ­¥å¹¶å‘è¯·æ±‚å¤„ç†
"""

import asyncio
import time
import logging
import uuid
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vllm_engine_helper import VLLMEngineManager
from config.Config import GLOBAL_CONFIG
from util.PromptLoader import PromptLoader
from vllm import SamplingParams

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_sampling_params(max_tokens=256):
    """åˆ›å»ºé‡‡æ ·å‚æ•°"""
    return SamplingParams(
        temperature=0.8,
        top_p=0.95,
        max_tokens=max_tokens,
        stop=None
    )


async def collect_generation_output_with_timeout(engine, prompt, sampling_params, request_id, timeout=30):
    """æ”¶é›†ç”Ÿæˆè¾“å‡º - å¸¦è¶…æ—¶æ§åˆ¶"""
    try:
        start_time = time.time()
        logger.info(f"å¼€å§‹å¤„ç†è¯·æ±‚: {request_id}")
        
        # ä½¿ç”¨asyncio.wait_foræ·»åŠ è¶…æ—¶æ§åˆ¶
        async def generate_with_timeout():
            results = []
            async for request_output in engine.generate(prompt, sampling_params, request_id):
                results.append(request_output)
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if time.time() - start_time > timeout:
                    logger.warning(f"è¯·æ±‚ {request_id} å¤„ç†è¶…æ—¶ ({timeout}s)ï¼Œåœæ­¢ç”Ÿæˆ")
                    break
            return results
        
        try:
            results = await asyncio.wait_for(generate_with_timeout(), timeout=timeout)
        except asyncio.TimeoutError:
            logger.warning(f"è¯·æ±‚ {request_id} è¶…æ—¶ ({timeout}s)")
            return {
                'request_id': request_id,
                'status': 'timeout',
                'total_time': timeout,
                'output_tokens': 0
            }
        
        end_time = time.time()
        total_time = end_time - start_time
        
        if results:
            final_output = results[-1]
            output_text = final_output.outputs[0].text if final_output.outputs else ""
            output_tokens = len(final_output.outputs[0].token_ids) if final_output.outputs else 0
            
            logger.info(f"å®Œæˆè¯·æ±‚: {request_id}, è€—æ—¶: {total_time:.3f}s, è¾“å‡ºtokens: {output_tokens}")
            return {
                'request_id': request_id,
                'status': 'completed',
                'total_time': total_time,
                'output_tokens': output_tokens,
                'output_text': output_text[:100] + "..." if len(output_text) > 100 else output_text
            }
        else:
            logger.warning(f"è¯·æ±‚ {request_id} æ²¡æœ‰äº§ç”Ÿè¾“å‡º")
            return {
                'request_id': request_id,
                'status': 'no_output',
                'total_time': total_time,
                'output_tokens': 0
            }
            
    except Exception as e:
        logger.error(f"è¯·æ±‚ {request_id} å¤„ç†å¤±è´¥: {e}")
        return {
            'request_id': request_id,
            'status': 'error',
            'total_time': time.time() - start_time,
            'output_tokens': 0,
            'error': str(e)
        }


async def monitor_vllm_scheduler_with_round_time(engine, round_time=30, interval=0.5):
    """ç›‘æ§vLLMè°ƒåº¦å™¨çŠ¶æ€ - å¸¦round timeé™åˆ¶"""
    logger.info(f"å¼€å§‹ç›‘æ§vLLMè°ƒåº¦å™¨çŠ¶æ€ï¼Œround time: {round_time}s...")
    
    start_time = time.time()
    max_waiting = 0
    max_running = 0
    max_swapped = 0
    total_samples = 0
    monitoring_data = []
    
    while time.time() - start_time < round_time:
        try:
            if hasattr(engine, 'engine') and hasattr(engine.engine, 'scheduler'):
                scheduler = engine.engine.scheduler
                waiting_count = len(scheduler.waiting) if hasattr(scheduler, 'waiting') else 0
                running_count = len(scheduler.running) if hasattr(scheduler, 'running') else 0
                swapped_count = len(scheduler.swapped) if hasattr(scheduler, 'swapped') else 0
                
                max_waiting = max(max_waiting, waiting_count)
                max_running = max(max_running, running_count)
                max_swapped = max(max_swapped, swapped_count)
                total_samples += 1
                
                elapsed = time.time() - start_time
                remaining = round_time - elapsed
                
                monitoring_data.append({
                    'elapsed': elapsed,
                    'waiting': waiting_count,
                    'running': running_count,
                    'swapped': swapped_count
                })
                
                logger.info(f"[{elapsed:.1f}s/{round_time}s] vLLMè°ƒåº¦å™¨ - ç­‰å¾…: {waiting_count}, è¿è¡Œ: {running_count}, äº¤æ¢: {swapped_count}, å‰©ä½™: {remaining:.1f}s")
                
                # Round timeç»“æŸå‰5ç§’å¼€å§‹è­¦å‘Š
                if remaining <= 5 and remaining > 0:
                    total_active = waiting_count + running_count + swapped_count
                    if total_active > 0:
                        logger.warning(f"Round timeå³å°†ç»“æŸï¼Œä»æœ‰ {total_active} ä¸ªè¯·æ±‚åœ¨å¤„ç†ä¸­")
                        
            else:
                logger.warning("æ— æ³•è®¿é—®vLLMè°ƒåº¦å™¨")
                
        except Exception as e:
            logger.debug(f"ç›‘æ§è°ƒåº¦å™¨çŠ¶æ€æ—¶å‡ºé”™: {e}")
            
        await asyncio.sleep(interval)
    
    # Round timeç»“æŸåæ£€æŸ¥å‰©ä½™è¯·æ±‚
    try:
        if hasattr(engine, 'engine') and hasattr(engine.engine, 'scheduler'):
            scheduler = engine.engine.scheduler
            final_waiting = len(scheduler.waiting) if hasattr(scheduler, 'waiting') else 0
            final_running = len(scheduler.running) if hasattr(scheduler, 'running') else 0
            final_swapped = len(scheduler.swapped) if hasattr(scheduler, 'swapped') else 0
            total_remaining = final_waiting + final_running + final_swapped
            
            logger.info(f"Round timeç»“æŸ - æœªå®Œæˆè¯·æ±‚: ç­‰å¾…={final_waiting}, è¿è¡Œ={final_running}, äº¤æ¢={final_swapped}, æ€»è®¡={total_remaining}")
    except Exception as e:
        logger.debug(f"æ£€æŸ¥æœ€ç»ˆçŠ¶æ€å¤±è´¥: {e}")
    
    logger.info(f"ç›‘æ§å®Œæˆ - æœ€å¤§ç­‰å¾…: {max_waiting}, æœ€å¤§è¿è¡Œ: {max_running}, æœ€å¤§äº¤æ¢: {max_swapped}, æ€»é‡‡æ ·: {total_samples}")
    return {
        'max_waiting': max_waiting,
        'max_running': max_running, 
        'max_swapped': max_swapped,
        'total_samples': total_samples,
        'monitoring_data': monitoring_data
    }


async def test_vllm_scheduler_with_round_time():
    """æµ‹è¯•vLLMè°ƒåº¦å™¨åœ¨round timeé™åˆ¶ä¸‹çš„è¡¨ç°"""
    logger.info("=== æµ‹è¯•vLLMè°ƒåº¦å™¨ + Round Timeé™åˆ¶ ===")
    
    # å¯åŠ¨vLLMå¼•æ“
    engine_manager = VLLMEngineManager()
    try:
        logger.info("å¯åŠ¨vLLMå¼•æ“...")
        engine = await engine_manager.create_engine(
            model_path="/home/llm/model_hub/Llama-3.1-8B",
            max_num_seqs=8,  # å…è®¸8ä¸ªå¹¶å‘åºåˆ—
            tensor_parallel_size=8,
            suppress_logs=True
        )
        
        logger.info("âœ“ vLLMå¼•æ“å¯åŠ¨æˆåŠŸ")
        
        # å‡†å¤‡æµ‹è¯•prompts - ä½¿ç”¨è¾ƒé•¿çš„promptç¡®ä¿å¤„ç†æ—¶é—´
        prompts = [
            "è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ï¼ŒåŒ…æ‹¬ä»æ—©æœŸç¬¦å·ä¸»ä¹‰åˆ°ç°ä»£æ·±åº¦å­¦ä¹ çš„æ¼”å˜è¿‡ç¨‹ï¼Œä»¥åŠå„ä¸ªé˜¶æ®µçš„å…³é”®æŠ€æœ¯çªç ´å’Œä»£è¡¨æ€§æˆæœ",
            "åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨ï¼Œè¯¦ç»†æè¿°å·ç§¯ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ï¼Œå¹¶ä¸¾ä¾‹è¯´æ˜åœ¨å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­çš„å…·ä½“å®ç°æ–¹æ³•",
            "è®¨è®ºè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹ä»‹ç»Transformeræ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶çš„åŸç†ï¼Œä»¥åŠåœ¨æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœ",
            "è§£é‡Šå¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œç®—æ³•åŸç†ï¼ŒåŒ…æ‹¬Q-learningã€ç­–ç•¥æ¢¯åº¦ç­‰æ–¹æ³•ï¼Œå¹¶åˆ†æå…¶åœ¨æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸçš„æˆåŠŸæ¡ˆä¾‹",
            "æè¿°å¤§æ•°æ®å¤„ç†å’Œåˆ†ææŠ€æœ¯çš„å‘å±•è¶‹åŠ¿ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è®¡ç®—ã€æµå¤„ç†ã€å®æ—¶åˆ†æç­‰å…³é”®æŠ€æœ¯ï¼Œä»¥åŠåœ¨å•†ä¸šæ™ºèƒ½å’Œå†³ç­–æ”¯æŒä¸­çš„åº”ç”¨",
            "åˆ†æäº‘è®¡ç®—å’Œè¾¹ç¼˜è®¡ç®—çš„æŠ€æœ¯ç‰¹ç‚¹ï¼Œæ¯”è¾ƒå…¶åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„ä¼˜åŠ£åŠ¿ï¼Œå¹¶æ¢è®¨æœªæ¥è®¡ç®—æ¶æ„çš„å‘å±•æ–¹å‘å’ŒæŠ€æœ¯æŒ‘æˆ˜",
            "ä»‹ç»åŒºå—é“¾æŠ€æœ¯çš„æ ¸å¿ƒåŸç†å’Œå…±è¯†æœºåˆ¶ï¼Œåˆ†æå…¶åœ¨é‡‘èç§‘æŠ€ã€ä¾›åº”é“¾ç®¡ç†ã€æ•°å­—èº«ä»½ç­‰é¢†åŸŸçš„åˆ›æ–°åº”ç”¨å’Œå‘å±•å‰æ™¯",
            "è®¨è®ºé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å’ŒæŠ€æœ¯ä¼˜åŠ¿ï¼Œè§£é‡Šé‡å­æ¯”ç‰¹ã€é‡å­çº ç¼ ç­‰æ¦‚å¿µï¼Œå¹¶åˆ†æé‡å­è®¡ç®—åœ¨å¯†ç å­¦ã€ä¼˜åŒ–é—®é¢˜ç­‰æ–¹é¢çš„æ½œåœ¨å½±å“",
            "æ¢è®¨ç‰©è”ç½‘æŠ€æœ¯çš„æ¶æ„è®¾è®¡å’Œå…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬ä¼ æ„Ÿå™¨ç½‘ç»œã€é€šä¿¡åè®®ã€æ•°æ®å¤„ç†ç­‰ï¼Œä»¥åŠåœ¨æ™ºæ…§åŸå¸‚å»ºè®¾ä¸­çš„å…·ä½“åº”ç”¨æ¡ˆä¾‹",
            "åˆ†æç½‘ç»œå®‰å…¨å¨èƒçš„æ¼”å˜è¶‹åŠ¿å’Œé˜²æŠ¤ç­–ç•¥ï¼ŒåŒ…æ‹¬æ¶æ„è½¯ä»¶æ£€æµ‹ã€å…¥ä¾µé˜²æŠ¤ã€æ•°æ®åŠ å¯†ç­‰æŠ€æœ¯ï¼Œä»¥åŠå®‰å…¨æ²»ç†çš„æœ€ä½³å®è·µæ–¹æ³•"
        ]
        
        # è®¾ç½®å‚æ•°
        round_time = 30  # 30ç§’round time
        request_timeout = round_time - 5  # è¯·æ±‚è¶…æ—¶æ—¶é—´æ¯”round timeçŸ­5ç§’
        sampling_params = create_sampling_params(max_tokens=200)
        
        logger.info(f"Round Time: {round_time}s, è¯·æ±‚è¶…æ—¶: {request_timeout}s")
        logger.info(f"å‡†å¤‡æäº¤ {len(prompts)} ä¸ªè¯·æ±‚...")
        
        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        monitor_task = asyncio.create_task(
            monitor_vllm_scheduler_with_round_time(engine, round_time=round_time, interval=0.5)
        )
        
        # å¿«é€Ÿè¿ç»­æäº¤æ‰€æœ‰è¯·æ±‚
        tasks = []
        submit_start = time.time()
        
        for i, prompt in enumerate(prompts):
            request_id = f"round_test_{i}_{uuid.uuid4().hex[:6]}"
            
            task = asyncio.create_task(
                collect_generation_output_with_timeout(engine, prompt, sampling_params, request_id, timeout=request_timeout)
            )
            tasks.append(task)
            
            logger.info(f"æäº¤è¯·æ±‚ {i+1}: {request_id}")
            await asyncio.sleep(0.1)  # å¿«é€Ÿæäº¤
        
        submit_time = time.time() - submit_start
        logger.info(f"âœ“ æ‰€æœ‰è¯·æ±‚æäº¤å®Œæˆï¼Œè€—æ—¶: {submit_time:.3f}s")
        
        # ç­‰å¾…round timeç»“æŸæˆ–æ‰€æœ‰è¯·æ±‚å®Œæˆï¼ˆä»¥å…ˆåˆ°çš„ä¸ºå‡†ï¼‰
        logger.info(f"ç­‰å¾…round time ({round_time}s) ç»“æŸ...")
        
        try:
            # ä½¿ç”¨asyncio.waitåœ¨round timeå†…ç­‰å¾…ä»»åŠ¡å®Œæˆ
            done, pending = await asyncio.wait(
                tasks, 
                timeout=round_time,
                return_when=asyncio.ALL_COMPLETED
            )
            
            if pending:
                logger.warning(f"Round timeç»“æŸï¼Œä»æœ‰ {len(pending)} ä¸ªè¯·æ±‚æœªå®Œæˆï¼Œå–æ¶ˆè¿™äº›è¯·æ±‚")
                for task in pending:
                    task.cancel()
                
                # ç­‰å¾…å–æ¶ˆçš„ä»»åŠ¡å®Œæˆ
                await asyncio.gather(*pending, return_exceptions=True)
                
        except Exception as e:
            logger.error(f"ç­‰å¾…è¯·æ±‚å®Œæˆæ—¶å‡ºé”™: {e}")
        
        # æ”¶é›†ç›‘æ§ç»“æœ
        monitor_task.cancel()
        try:
            monitor_stats = await monitor_task
        except asyncio.CancelledError:
            monitor_stats = {'max_waiting': 0, 'max_running': 0, 'max_swapped': 0, 'total_samples': 0}
        
        # åˆ†æç»“æœ
        completed_results = []
        for task in tasks:
            if task.done() and not task.cancelled():
                try:
                    result = task.result()
                    if result:
                        completed_results.append(result)
                except Exception as e:
                    logger.debug(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
        
        successful_results = [r for r in completed_results if r.get('status') == 'completed']
        timeout_results = [r for r in completed_results if r.get('status') == 'timeout']
        error_results = [r for r in completed_results if r.get('status') == 'error']
        
        logger.info("=== Round Timeæµ‹è¯•ç»“æœåˆ†æ ===")
        logger.info(f"Round Time: {round_time}s")
        logger.info(f"æ€»è¯·æ±‚æ•°: {len(prompts)}")
        logger.info(f"æˆåŠŸå®Œæˆ: {len(successful_results)}")
        logger.info(f"è¶…æ—¶è¯·æ±‚: {len(timeout_results)}")
        logger.info(f"é”™è¯¯è¯·æ±‚: {len(error_results)}")
        logger.info(f"æœªå®Œæˆè¯·æ±‚: {len(tasks) - len(completed_results)}")
        logger.info(f"è¯·æ±‚æäº¤æ—¶é—´: {submit_time:.3f}s")
        logger.info(f"vLLMæœ€å¤§ç­‰å¾…é˜Ÿåˆ—: {monitor_stats['max_waiting']}")
        logger.info(f"vLLMæœ€å¤§è¿è¡Œé˜Ÿåˆ—: {monitor_stats['max_running']}")
        logger.info(f"vLLMæœ€å¤§äº¤æ¢é˜Ÿåˆ—: {monitor_stats['max_swapped']}")
        
        if successful_results:
            avg_time = sum(r['total_time'] for r in successful_results) / len(successful_results)
            avg_tokens = sum(r['output_tokens'] for r in successful_results) / len(successful_results)
            logger.info(f"æˆåŠŸè¯·æ±‚å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.3f}s")
            logger.info(f"æˆåŠŸè¯·æ±‚å¹³å‡è¾“å‡ºtokens: {avg_tokens:.1f}")
        
        # éªŒè¯round timeæ§åˆ¶æ•ˆæœ
        assert monitor_stats['max_waiting'] > 0, "vLLMç­‰å¾…é˜Ÿåˆ—åº”è¯¥æœ‰è¯·æ±‚æ’é˜Ÿ"
        assert monitor_stats['max_running'] > 0, "vLLMè¿è¡Œé˜Ÿåˆ—åº”è¯¥æœ‰è¯·æ±‚åœ¨å¤„ç†"
        assert len(completed_results) > 0, "åº”è¯¥è‡³å°‘æœ‰ä¸€äº›è¯·æ±‚å®Œæˆ"
        
        # è®¡ç®—ååé‡
        throughput = len(successful_results) / round_time
        logger.info(f"ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        
        logger.info("âœ“ Round Timeæ§åˆ¶æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ¸…ç†èµ„æº
        await engine_manager.cleanup()


async def test_queue_behavior_under_pressure():
    """æµ‹è¯•é«˜è´Ÿè½½ä¸‹çš„é˜Ÿåˆ—è¡Œä¸º"""
    logger.info("=== æµ‹è¯•é«˜è´Ÿè½½ä¸‹çš„é˜Ÿåˆ—è¡Œä¸º ===")
    
    engine_manager = VLLMEngineManager()
    try:
        # ä½¿ç”¨è¾ƒå°çš„max_num_seqsæ¥åˆ¶é€ é˜Ÿåˆ—å‹åŠ›
        engine = await engine_manager.create_engine(
            model_path="/home/llm/model_hub/Llama-3.1-8B",
            max_num_seqs=4,  # åªå…è®¸4ä¸ªå¹¶å‘
            tensor_parallel_size=8,
            suppress_logs=True
        )
        
        logger.info("âœ“ vLLMå¼•æ“å¯åŠ¨æˆåŠŸ (max_num_seqs=4)")
        
        # åˆ›å»ºçŸ­promptç¡®ä¿å¿«é€Ÿå¤„ç†
        short_prompts = [f"è®¡ç®— {i} + {i+1} = ?" for i in range(15)]  # 15ä¸ªç®€çŸ­è¯·æ±‚
        
        round_time = 20  # 20ç§’round time
        sampling_params = create_sampling_params(max_tokens=50)  # çŸ­è¾“å‡º
        
        # å¯åŠ¨ç›‘æ§
        monitor_task = asyncio.create_task(
            monitor_vllm_scheduler_with_round_time(engine, round_time=round_time, interval=0.3)
        )
        
        # å¿«é€Ÿæäº¤æ‰€æœ‰è¯·æ±‚
        tasks = []
        for i, prompt in enumerate(short_prompts):
            request_id = f"pressure_test_{i}_{uuid.uuid4().hex[:4]}"
            
            task = asyncio.create_task(
                collect_generation_output_with_timeout(engine, prompt, sampling_params, request_id, timeout=15)
            )
            tasks.append(task)
            
            logger.info(f"æäº¤å‹åŠ›æµ‹è¯•è¯·æ±‚ {i+1}: {request_id}")
            await asyncio.sleep(0.05)  # æå¿«æäº¤é—´éš”
        
        logger.info("è§‚å¯Ÿé˜Ÿåˆ—å‹åŠ›å’Œå¤„ç†æ•ˆç‡...")
        
        # ç­‰å¾…round time
        done, pending = await asyncio.wait(tasks, timeout=round_time)
        
        if pending:
            logger.warning(f"å‹åŠ›æµ‹è¯•ä¸­æœ‰ {len(pending)} ä¸ªè¯·æ±‚æœªå®Œæˆ")
            for task in pending:
                task.cancel()
            await asyncio.gather(*pending, return_exceptions=True)
        
        # åœæ­¢ç›‘æ§
        monitor_task.cancel()
        try:
            monitor_stats = await monitor_task
        except asyncio.CancelledError:
            monitor_stats = {'max_waiting': 0, 'max_running': 0, 'max_swapped': 0}
        
        completed_count = len([t for t in tasks if t.done() and not t.cancelled()])
        
        logger.info("=== å‹åŠ›æµ‹è¯•ç»“æœ ===")
        logger.info(f"è¯·æ±‚æ€»æ•°: {len(short_prompts)}")
        logger.info(f"å®Œæˆè¯·æ±‚: {completed_count}")
        logger.info(f"æœ€å¤§ç­‰å¾…é˜Ÿåˆ—: {monitor_stats['max_waiting']}")
        logger.info(f"æœ€å¤§è¿è¡Œé˜Ÿåˆ—: {monitor_stats['max_running']}")
        logger.info(f"å®Œæˆç‡: {completed_count/len(short_prompts)*100:.1f}%")
        
        # éªŒè¯å‹åŠ›æµ‹è¯•æ•ˆæœ
        assert monitor_stats['max_running'] <= 4, f"è¿è¡Œé˜Ÿåˆ—ä¸åº”è¶…è¿‡max_num_seqs: {monitor_stats['max_running']}"
        assert monitor_stats['max_waiting'] >= 5, "ç­‰å¾…é˜Ÿåˆ—åº”è¯¥æœ‰æ˜æ˜¾å †ç§¯"
        assert completed_count >= len(short_prompts) * 0.5, "è‡³å°‘åº”è¯¥å®Œæˆ50%çš„è¯·æ±‚"
        
        logger.info("âœ“ å‹åŠ›æµ‹è¯•é€šè¿‡")
        
    finally:
        await engine_manager.cleanup()


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("å¼€å§‹è¿è¡ŒvLLMè°ƒåº¦å™¨é˜Ÿåˆ—ç›‘æ§æµ‹è¯•å¥—ä»¶ï¼ˆå¸¦Round Timeæ§åˆ¶ï¼‰")
    
    try:
        await test_vllm_scheduler_with_round_time()
        await test_queue_behavior_under_pressure()
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼vLLMè°ƒåº¦å™¨åœ¨Round Timeé™åˆ¶ä¸‹å·¥ä½œæ­£å¸¸")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    asyncio.run(run_all_tests()) 