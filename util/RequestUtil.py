import asyncio
import time
from datetime import datetime
from typing import Any
import uuid

import numpy as np
import logging

import random
import threading
from vllm import SamplingParams
from config.Config import GLOBAL_CONFIG
from util.ThreadSafeUtil import ThreadSafeCounter

# å¯¼å…¥å”¯ä¸€request_idç”Ÿæˆå‡½æ•°
try:
    from RequestQueueManager.RequestQueueManager import generate_unique_request_id
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›å¤‡ç”¨å®ç°
    _request_counter = 0
    _counter_lock = threading.Lock()
    
    def generate_unique_request_id(client_id: str, worker_id: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚IDï¼Œé¿å…é‡å¤"""
        global _request_counter
        with _counter_lock:
            _request_counter += 1
            counter = _request_counter
        
        # ä½¿ç”¨UUIDç¡®ä¿å…¨å±€å”¯ä¸€æ€§ï¼Œå¹¶æ·»åŠ å¯è¯»çš„å‰ç¼€
        unique_id = str(uuid.uuid4())[:8]  # å–UUIDçš„å‰8ä½
        timestamp = int(time.time() * 1000000)  # ä½¿ç”¨å¾®ç§’æ—¶é—´æˆ³
        
        return f"req_{client_id}_{worker_id}_{counter}_{timestamp}_{unique_id}"


async def process_stream(stream):
    first_token_time = None
    total_tokens = 0
    async for chunk in stream:
        if first_token_time is None:
            first_token_time = time.time()
        if chunk.choices[0].delta.content:
            total_tokens += 1
        if chunk.choices[0].finish_reason is not None:
            break
    return first_token_time, total_tokens


async def make_request(openai, experiment, request, start_time=None, request_id=None, priority=None):
    """
    å‘é€è¯·æ±‚ - è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨ç›´æ¥å¼•æ“æˆ–HTTPå®¢æˆ·ç«¯
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥çš„vLLMå¼•æ“
    if 'vllm_engine' in GLOBAL_CONFIG and GLOBAL_CONFIG['vllm_engine'] is not None:
        # ä½¿ç”¨ç›´æ¥å¼•æ“API
        return await make_request_direct_engine(GLOBAL_CONFIG['vllm_engine'], experiment, request, start_time,
                                                request_id, priority)
    else:
        # ä½¿ç”¨HTTPå®¢æˆ·ç«¯ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
        return await make_request_http_client(openai, experiment, request, start_time, request_id)


async def make_request_direct_engine(engine, experiment, request, start_time=None, request_id=None, priority=None):
    """
    ç›´æ¥ä½¿ç”¨AsyncLLMEngineå¤„ç†è¯·æ±‚
    
    Args:
        engine: AsyncLLMEngineå®ä¾‹
        experiment: å®éªŒå¯¹è±¡
        request: è¯·æ±‚å†…å®¹
        start_time: å¼€å§‹æ—¶é—´
        request_id: è¯·æ±‚IDï¼ˆå¦‚æœæä¾›åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ç”Ÿæˆæ–°çš„ï¼‰
        
    Returns:
        tuple: (output_tokens, elapsed_time, tokens_per_second, ttft, input_tokens, slo_met)
        :param priority:
    """
    if start_time is None:
        start_time = time.time()

    # å¦‚æœæ²¡æœ‰æä¾›request_idï¼Œç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ID
    if request_id is None:
        client_id = getattr(experiment.client, 'client_id', 'unknown_client')
        worker_id = getattr(experiment, 'worker_id', 'unknown_worker')
        request_id = generate_unique_request_id(client_id, worker_id)

    try:
        # æ³¨å†Œè¯·æ±‚IDåˆ°å®éªŒçš„å®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'register_request_id'):
            experiment.client.register_request_id(request_id)

        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        client_id = getattr(experiment.client, 'client_id', 'unknown_client')
        experiment.logger.debug(f"Client {client_id}: å¼€å§‹å¤„ç†è¯·æ±‚ {request_id}")

        # ğŸ”¥ æ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼ï¼šå­—å…¸ï¼ˆæå–promptï¼‰æˆ–å­—ç¬¦ä¸²
        actual_prompt = request.get('prompt', request) if isinstance(request, dict) else request

        # å‡†å¤‡è¯·æ±‚å‚æ•°
        sampling_params = SamplingParams(
            n=1,
            temperature=0.8,
            top_p=0.95,
            max_tokens=experiment.output_tokens
        )

        # å‘é€è¯·æ±‚åˆ°å¼•æ“ - æ³¨æ„è¿™é‡Œè¿”å›çš„æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨
        request_generator = engine.generate(actual_prompt, sampling_params, request_id)

        # ä½¿ç”¨async forè¿­ä»£å¼‚æ­¥ç”Ÿæˆå™¨è·å–æœ€ç»ˆç»“æœï¼Œå¹¶è®¾ç½®è¶…æ—¶
        request_output = None
        first_chunk_time = None

        # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œä½¿ç”¨experimentçš„request_timeout
        timeout_seconds = getattr(experiment, 'request_timeout', 30.0)
        
        # åˆ›å»ºä¸€ä¸ªå¼‚æ­¥å‡½æ•°æ¥å¤„ç†ç”Ÿæˆå™¨è¿­ä»£
        async def process_generator():
            nonlocal request_output, first_chunk_time
            async for output_chunk in request_generator:
                if first_chunk_time is None:
                    first_chunk_time = time.time()
                request_output = output_chunk  # ä¿ç•™æœ€åä¸€ä¸ªè¾“å‡ºä½œä¸ºæœ€ç»ˆç»“æœ
        
        try:
            # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¶…æ—¶
            await asyncio.wait_for(process_generator(), timeout=timeout_seconds)
        except asyncio.TimeoutError:
            experiment.logger.warning(f"Client {client_id}: è¯·æ±‚ {request_id} è¶…æ—¶ ({timeout_seconds}s)")
            if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
                experiment.client.unregister_request_id(request_id)
            return None
        except asyncio.CancelledError:
            # è¯·æ±‚è¢«å–æ¶ˆï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼ˆå®éªŒç»“æŸæ—¶ï¼‰
            experiment.logger.debug(f"Client {client_id}: è¯·æ±‚ {request_id} è¢«å–æ¶ˆ (å®éªŒç»“æŸæ—¶çš„æ­£å¸¸æ¸…ç†)")
            if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
                experiment.client.unregister_request_id(request_id)
            return None

        end_time = time.time()

        # è®¡ç®—é¦–ä¸ªtokenæ—¶é—´ (TTFT)
        ttft_ms = None
        if first_chunk_time:
            ttft_ms = (first_chunk_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

        # å¦‚æœæ²¡æœ‰è·å–åˆ°ä»»ä½•è¾“å‡ºï¼Œè¿”å›None
        if request_output is None:
            experiment.logger.warning(f"Client {client_id}: è¯·æ±‚ {request_id} æ²¡æœ‰è¿”å›ä»»ä½•è¾“å‡º")
            if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
                experiment.client.unregister_request_id(request_id)
            return None

        # è·å–è¾“å‡ºå†…å®¹
        output_text = ""
        input_tokens = 0
        output_tokens = 0

        if hasattr(request_output, 'outputs') and request_output.outputs:
            output = request_output.outputs[0]
            output_text = output.text
            output_tokens = len(output.token_ids) if hasattr(output, 'token_ids') else 0

        # è·å–è¾“å…¥tokenæ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(request_output, 'prompt_token_ids'):
            input_tokens = len(request_output.prompt_token_ids)
        elif hasattr(request_output, 'inputs') and hasattr(request_output.inputs, 'token_ids'):
            input_tokens = len(request_output.inputs.token_ids)

        elapsed_time = end_time - start_time
        tokens_per_second = output_tokens / elapsed_time if elapsed_time > 0 else 0

        # æ£€æŸ¥SLO - ç»Ÿä¸€ä½¿ç”¨ç§’è¿›è¡Œæ¯”è¾ƒ
        # elapsed_timeæ˜¯ç§’ï¼Œlatency_sloä¹Ÿæ˜¯ç§’
        slo_met = elapsed_time <= experiment.latency_slo  # éƒ½ä½¿ç”¨ç§’ä¸ºå•ä½

        # æ·»åŠ å®Œæˆæ—¥å¿—å’ŒSLOæ£€æŸ¥æ—¥å¿—ï¼ˆé‡‡æ ·è®°å½•ï¼Œé¿å…æ—¥å¿—è¿‡å¤šï¼‰
        # æ¯100ä¸ªè¯·æ±‚è®°å½•ä¸€æ¬¡ï¼Œæˆ–è€…è¿åSLOæ—¶è®°å½•
        should_log = (not slo_met) or (hash(request_id) % 100 == 0)
        if should_log:
            experiment.logger.debug(
                f"Client {client_id}: è¯·æ±‚ {request_id[-8:]} - è€—æ—¶: {elapsed_time:.3f}s, "
                f"SLOé˜ˆå€¼: {experiment.latency_slo}s, SLO{'æ»¡è¶³' if slo_met else 'è¿å'}")
        else:
            experiment.logger.debug(
                f"Client {client_id}: å®Œæˆè¯·æ±‚ {request_id}, è€—æ—¶: {elapsed_time:.3f}s, "
                f"SLO: {experiment.latency_slo}s, SLOè¿å: {not slo_met}")

        # å–æ¶ˆæ³¨å†Œè¯·æ±‚ID
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
            experiment.client.unregister_request_id(request_id)

        return output_tokens, elapsed_time, tokens_per_second, ttft_ms, input_tokens, 1 if slo_met else 0

    except asyncio.TimeoutError:
        end_time = time.time()
        # è®°å½•timeoutæ¬¡æ•°
        if hasattr(experiment, 'timeout_count'):
            experiment.timeout_count += 1

        # è¶…æ—¶æ—¶ä¹Ÿè¦æ³¨é”€è¯·æ±‚ID
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
            experiment.client.unregister_request_id(request_id)

        experiment.logger.warning(
            f"Client {client_id}: è¯·æ±‚ {request_id} è¶…æ—¶ ({end_time - start_time:.3f}s) (Total timeouts: {experiment.timeout_count})")

        return None
    except Exception as e:
        # å¼‚å¸¸æ—¶ä¹Ÿè¦æ³¨é”€è¯·æ±‚ID
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
            experiment.client.unregister_request_id(request_id)

        experiment.logger.error(f"Error during direct engine request: {str(e)}")
        return None


async def make_request_http_client(client, experiment, request, start_time=None, request_id=None):
    """
    ä½¿ç”¨HTTPå®¢æˆ·ç«¯å¤„ç†è¯·æ±‚ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
    """
    if start_time is None:
        start_time = time.time()

    # è·å–å®¢æˆ·ç«¯ID
    client_id = getattr(experiment.client, 'client_id', 'unknown_client')
    worker_id = getattr(experiment, 'worker_id', 'unknown_worker')

    # å¦‚æœæ²¡æœ‰æä¾›request_idï¼Œç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„
    if request_id is None:
        request_id = generate_unique_request_id(client_id, worker_id)

    try:
        # æ³¨å†Œè¯·æ±‚IDåˆ°å®éªŒçš„å®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'register_request_id'):
            experiment.client.register_request_id(request_id)

        # ğŸ”¥ æ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼ï¼šå­—å…¸ï¼ˆæå–promptï¼‰æˆ–å­—ç¬¦ä¸²
        actual_prompt = request.get('prompt', request) if isinstance(request, dict) else request

        # ä½¿ç”¨log_request=Falseå‚æ•°æ¥ç¦æ­¢åœ¨æ—¥å¿—ä¸­æ‰“å°è¯·æ±‚å†…å®¹
        stream = await client.chat.completions.create(
            model=GLOBAL_CONFIG['request_model_name'],
            messages=[{"role": "user", "content": actual_prompt}],
            max_tokens=experiment.output_tokens,
            stream=True
            # æ³¨æ„ï¼šç§»é™¤ extra_headersï¼Œå› ä¸º OpenAI å®¢æˆ·ç«¯å¯èƒ½ä¸æ”¯æŒ
            # è¯·æ±‚IDä»ç„¶ä¼šè¢«è·Ÿè¸ªï¼Œä½†ä¸ä¼šé€šè¿‡headerä¼ é€’ç»™æœåŠ¡å™¨
        )
        first_token_time, output_tokens = await asyncio.wait_for(process_stream(stream),
                                                                 timeout=experiment.request_timeout)
        end_time = time.time()
        elapsed_time = end_time - start_time
        ttft = first_token_time - start_time if first_token_time else None
        input_token = experiment.tokenizer(request, truncation=False, return_tensors="pt").input_ids[0]
        tokens_per_second = output_tokens / elapsed_time if elapsed_time > 0 else 0

        # æ­£å¸¸å®Œæˆæ—¶æ³¨é”€è¯·æ±‚ID
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
            experiment.client.unregister_request_id(request_id)

        # æ£€æŸ¥SLO - ç»Ÿä¸€ä½¿ç”¨ç§’è¿›è¡Œæ¯”è¾ƒ
        # elapsed_timeæ˜¯ç§’ï¼Œlatency_sloä¹Ÿæ˜¯ç§’
        slo_met = elapsed_time <= experiment.latency_slo  # éƒ½ä½¿ç”¨ç§’ä¸ºå•ä½

        return output_tokens, elapsed_time, tokens_per_second, ttft, len(
            input_token), 1 if slo_met else 0

    except asyncio.TimeoutError:
        end_time = time.time()
        # è®°å½•timeoutæ¬¡æ•°
        if hasattr(experiment, 'timeout_count'):
            experiment.timeout_count += 1

        # è¶…æ—¶æ—¶ä¹Ÿè¦æ³¨é”€è¯·æ±‚ID
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
            experiment.client.unregister_request_id(request_id)

        experiment.logger.warning(
            f"Client {client_id}: è¯·æ±‚ {request_id} è¶…æ—¶ ({end_time - start_time:.3f}s) (Total timeouts: {experiment.timeout_count})")

        return None
    except Exception as e:
        # å¼‚å¸¸æ—¶ä¹Ÿè¦æ³¨é”€è¯·æ±‚ID
        if hasattr(experiment, 'client') and hasattr(experiment.client, 'unregister_request_id'):
            experiment.client.unregister_request_id(request_id)

        experiment.logger.error(f"Error during request: {str(e)}")
        return None


async def make_request_via_queue(queue_manager, client_id: str, worker_id: str,
                                 request_content: str, experiment, priority: int = 0, request_id: str = None) -> Any:
    """é€šè¿‡é˜Ÿåˆ—ç®¡ç†å™¨å‘é€è¯·æ±‚ - çœŸæ­£å¼‚æ­¥ç‰ˆæœ¬ï¼šåªæäº¤è¯·æ±‚ï¼Œä¸ç­‰å¾…å“åº”"""
    # å¦‚æœæ²¡æœ‰æä¾›request_idï¼Œç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„
    if request_id is None:
        request_id = generate_unique_request_id(client_id, worker_id)

    try:
        # åªæäº¤è¯·æ±‚åˆ°é˜Ÿåˆ—ï¼Œç«‹å³è¿”å›
        submitted_request_id = await queue_manager.submit_request(
            client_id=client_id,
            worker_id=worker_id,
            request_content=request_content,
            experiment=experiment,
            priority=priority,
            start_time=time.time(),
            request_id=request_id  # ä¼ é€’é¢„ç”Ÿæˆçš„request_id
        )

        # ä¸ç­‰å¾…å“åº”ï¼Œç›´æ¥è¿”å›ä¸€ä¸ªFutureæˆ–è€…ç‰¹æ®Šæ ‡è®°
        # è¿™æ ·è¯·æ±‚å°±èƒ½ç«‹å³æäº¤ï¼Œå®ç°çœŸæ­£çš„å¹¶å‘
        return submitted_request_id

    except Exception as e:
        experiment.logger.error(f"Error making request via queue: {e}")
        return None


async def collect_single_response(queue_manager, client_id, request_info, global_start_time, experiment):
    """æ”¶é›†å•ä¸ªè¯·æ±‚çš„å“åº”"""
    try:
        # è®¡ç®—åŠ¨æ€è¶…æ—¶æ—¶é—´
        current_elapsed = time.time() - global_start_time
        remaining_time = experiment.round_time - current_elapsed
        timeout = min(1000, max(5, remaining_time * 0.8))  # ä½¿ç”¨å‰©ä½™æ—¶é—´çš„80%ä½œä¸ºè¶…æ—¶
        
        result = await queue_manager.get_response(client_id, timeout=timeout)
        
        if result:
            request_info["status"] = "completed"
            request_info["end_time"] = time.time()
            return result, request_info
        else:
            request_info["status"] = "failed"
            request_info["end_time"] = time.time()
            return None, request_info
            
    except asyncio.TimeoutError:
        request_info["status"] = "timeout"
        request_info["end_time"] = time.time()
        return None, request_info
    except Exception as e:
        experiment.logger.error(f"Error collecting response for {request_info['request_id']}: {e}")
        request_info["status"] = "failed"
        request_info["end_time"] = time.time()
        return None, request_info


async def worker_with_queue(experiment, queue_manager, semaphore, results, worker_id, worker_json, qmp_per_worker):
    """ä½¿ç”¨é˜Ÿåˆ—ç®¡ç†å™¨çš„workerå‡½æ•° - æ‰¹é‡æäº¤æ¨¡å¼"""
    assert worker_json is not None, "sample_content is None!"
    assert isinstance(worker_json, list), f"sample_content is not a list! type={type(worker_json)}"
    assert len(worker_json) > 0, "sample_content is empty!"

    global_start_time = time.time()
    request_count = 0
    drift_time = 0
    completed = 0
    submitted_requests = []  # å­˜å‚¨å·²æäº¤çš„è¯·æ±‚ä¿¡æ¯

    # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„tokenè®¡æ•°å™¨
    tokens_counter = ThreadSafeCounter()

    # æ³¨å†Œå®¢æˆ·ç«¯åˆ°é˜Ÿåˆ—ç®¡ç†å™¨
    client_id = f"{experiment.client_id}_worker_{worker_id}"

    # è·å–å®¢æˆ·ç«¯IDç”¨äºæ—¥å¿—
    main_client_id = getattr(experiment.client, 'client_id', 'unknown_client')

    # é¢„å…ˆè®¡ç®—æ‰€æœ‰è¯·æ±‚çš„æ—¶é—´ç‚¹
    request_times = calculate_all_request_times(experiment, qmp_per_worker)

    # ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰æ—¶é—´ç‚¹æäº¤æ‰€æœ‰è¯·æ±‚åˆ°é˜Ÿåˆ—
    experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: Starting request submission phase")
    
    for target_time in request_times:
        if time.time() - global_start_time >= experiment.round_time:
            break
        current_time = time.time()
        if target_time <= current_time:
            # å¦‚æœç›®æ ‡æ—¶é—´å·²è¿‡ï¼Œç›´æ¥å‘é€è¯·æ±‚
            drift_time = current_time - target_time
        else:
            # å¦‚æœè¿˜æ²¡åˆ°ç›®æ ‡æ—¶é—´ï¼Œå…ˆsleep
            sleep_time = target_time - current_time
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
            else:
                experiment.logger.warning(
                    f"Client {main_client_id}: Warning: Negative sleep time detected: {sleep_time:.6f} seconds")
                continue

        # æäº¤è¯·æ±‚åˆ°é˜Ÿåˆ—ï¼ˆä¸ç­‰å¾…å“åº”ï¼‰
        request = random.choice(worker_json)
        priority = experiment.client.priority
        # ä½¿ç”¨å”¯ä¸€çš„request_idç”Ÿæˆå‡½æ•°
        request_id = generate_unique_request_id(main_client_id, f"worker_{worker_id}")

        # ç›´æ¥æäº¤è¯·æ±‚ï¼Œä¸ç­‰å¾…
        try:
            submitted_request_id = await queue_manager.submit_request(
                client_id=client_id,
                worker_id=f"worker_{worker_id}",
                request_content=request,
                experiment=experiment,
                priority=priority,
                start_time=time.time(),
                request_id=request_id
            )
            
            submitted_requests.append({
                "request_id": submitted_request_id,
                "submit_time": time.time(),
                "status": "submitted"
            })
            request_count += 1
            
        except Exception as e:
            experiment.logger.error(f"Error submitting request {request_id}: {e}")

    submission_time = time.time() - global_start_time
    experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: Submitted {request_count} requests in {submission_time:.2f}s")

    # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘æ”¶é›†æ‰€æœ‰å“åº”ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰
    experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: Starting response collection phase")
    
    collected_results = []
    
    # åˆ›å»ºæ”¶é›†ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å°è¯•æ”¶é›†ä¸€ä¸ªå“åº”
    collection_tasks = []
    for request_info in submitted_requests:
        task = asyncio.create_task(
            collect_single_response(queue_manager, client_id, request_info, global_start_time, experiment)
        )
        collection_tasks.append(task)
    
    # å¹¶å‘ç­‰å¾…æ‰€æœ‰æ”¶é›†ä»»åŠ¡ï¼Œä½†æœ‰æ€»ä½“è¶…æ—¶æ§åˆ¶
    start_collection_time = time.time()
    completed_tasks = 0
    
    while collection_tasks and completed_tasks < len(submitted_requests):
        current_elapsed = time.time() - global_start_time
        if current_elapsed >= experiment.round_time:
            experiment.logger.warning(f"Client {main_client_id} Worker {worker_id}: Round time exceeded during collection phase, stopping collection")
            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„æ”¶é›†ä»»åŠ¡
            for task in collection_tasks:
                if not task.done():
                    task.cancel()
            break
        
        # ç­‰å¾…ä»»ä½•ä¸€ä¸ªä»»åŠ¡å®Œæˆï¼Œæœ€å¤šç­‰å¾…1ç§’
        collection_timeout = min(1.0, experiment.round_time - current_elapsed)
        if collection_timeout <= 0:
            break
            
        try:
            done, pending = await asyncio.wait(
                collection_tasks, 
                timeout=collection_timeout,
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡
            for task in done:
                try:
                    result, req_info = await task
                    if result:
                        output_tokens = result[0]
                        new_total = tokens_counter.add(output_tokens)
                        collected_results.append(result)
                        completed += 1
                    
                    completed_tasks += 1
                except Exception as e:
                    experiment.logger.error(f"Error processing completed collection task: {e}")
                    completed_tasks += 1
            
            # æ›´æ–°ä»»åŠ¡åˆ—è¡¨ï¼Œç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
            collection_tasks = [task for task in collection_tasks if not task.done()]
            
        except asyncio.TimeoutError:
            # è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯æ£€æŸ¥round time
            continue
    
    # æ”¶é›†å®Œæˆåï¼Œæ£€æŸ¥å“ªäº›è¯·æ±‚æ²¡æœ‰è¢«æˆåŠŸæ”¶é›†
    for request_info in submitted_requests:
        # å¦‚æœæ²¡æœ‰end_timeï¼Œè¯´æ˜è¯·æ±‚æœªå®Œæˆï¼Œè®¾ç½®çŠ¶æ€å’Œç»“æŸæ—¶é—´
        if "end_time" not in request_info:
            request_info["status"] = "uncompleted"
            request_info["end_time"] = time.time()

    collection_time = time.time() - start_collection_time
    experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: Collection phase completed in {collection_time:.2f}s, collected {len(collected_results)} results")

    # å°†æ”¶é›†åˆ°çš„ç»“æœæ·»åŠ åˆ°resultsä¸­
    results.extend(collected_results)

    # å°†æœªå®Œæˆçš„è¯·æ±‚IDæ³¨å†Œåˆ°å®¢æˆ·ç«¯ï¼Œä¾›engine abortä½¿ç”¨
    if hasattr(experiment, 'client'):
        if not hasattr(experiment.client, 'active_request_ids'):
            experiment.client.active_request_ids = set()
        
        # æ·»åŠ æ‰€æœ‰éœ€è¦abortçš„è¯·æ±‚ID
        abort_request_ids = set()
        for req_info in submitted_requests:
            if req_info["status"] in ["submitted", "timeout", "failed", "uncompleted"]:
                abort_request_ids.add(req_info["request_id"])
        
        experiment.client.active_request_ids.update(abort_request_ids)
        experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: {len(abort_request_ids)} requests added to active list for abort")

    # ç­‰å¾…å‰©ä½™æ—¶é—´ï¼ˆå¦‚æœè¿˜æœ‰çš„è¯ï¼‰
    elapsed = time.time() - global_start_time
    remaining_time = experiment.round_time - elapsed
    if remaining_time > 0 and remaining_time <= experiment.round_time * GLOBAL_CONFIG.get('buffer_ratio', 0.5):
        await asyncio.sleep(remaining_time)
    elif remaining_time <= 0:
        experiment.logger.info(f"Client {main_client_id}: round time exceeded by {-remaining_time:.2f}s")
    else:
        experiment.logger.info(f"Client {main_client_id}: reached the end of the round time.")

    # è®¡ç®—æ€»è€—æ—¶
    total_elapsed_time = time.time() - global_start_time
    failed_count = len([r for r in submitted_requests if r["status"] == "failed"])
    timeout_count = len([r for r in submitted_requests if r["status"] == "timeout"])
    uncompleted_count = len([r for r in submitted_requests if r["status"] == "uncompleted"])

    experiment.logger.info(
        f"Client {main_client_id} Worker {worker_id}: Total requests: {request_count}, Completed: {completed}, Failed: {failed_count}, Timeout: {timeout_count}, Uncompleted: {uncompleted_count}")
    experiment.logger.info(
        f"Client {main_client_id} Worker {worker_id}: Success rate: {completed / len(submitted_requests) * 100:.2f}%" if submitted_requests else "No requests submitted")
    experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: Total tokens processed: {tokens_counter.value}")
    experiment.logger.info(
        f"Client {main_client_id} Worker {worker_id}: Total elapsed time: {total_elapsed_time:.2f} seconds, Round time: {experiment.round_time:.2f} seconds")
    
    # è®°å½•é˜¶æ®µæ—¶é—´åˆ°å®éªŒå¯¹è±¡ï¼ˆç”¨äºåç»­åˆ†æï¼‰
    if not hasattr(experiment, 'phase_timings'):
        experiment.phase_timings = []
    experiment.phase_timings.append({
        'worker_id': worker_id,
        'submission_time': submission_time,
        'collection_time': collection_time,
        'total_time': total_elapsed_time,
        'configured_round_time': experiment.round_time,
        'request_count': request_count,
        'completed': completed,
        'uncompleted': uncompleted_count
    })
    
    # æ‰“å°é˜¶æ®µæ—¶é—´æ‘˜è¦
    experiment.logger.info(f"Client {main_client_id} Worker {worker_id}: â±ï¸ Phase Timings:")
    experiment.logger.info(f"  - Submission phase: {submission_time:.2f}s")
    experiment.logger.info(f"  - Collection phase: {collection_time:.2f}s") 
    experiment.logger.info(f"  - Total elapsed: {total_elapsed_time:.2f}s (configured: {experiment.round_time:.0f}s)")

    return completed, drift_time, request_count


def calculate_all_request_times(experiment, qmp_per_worker):
    """
    é¢„å…ˆè®¡ç®—æ‰€æœ‰è¯·æ±‚çš„æ—¶é—´ç‚¹
    
    Args:
        experiment: å®éªŒå¯¹è±¡ï¼ŒåŒ…å«round_time, distribution, time_ratioç­‰å±æ€§
        qmp_per_worker: æ¯ä¸ªworkeræ¯åˆ†é’Ÿå‘é€çš„è¯·æ±‚æ•°é‡
    
    Returns:
        list: è¯·æ±‚æ—¶é—´ç‚¹åˆ—è¡¨
    """
    # ä»experimentå¯¹è±¡ä¸­è·å–å‚æ•°
    rate_lambda = qmp_per_worker
    round_time = experiment.round_time
    distribution = experiment.distribution
    time_ratio = experiment.time_ratio

    # é¢„ç•™ç¼“å†²æ—¶é—´ç»™æœ€åçš„è¯·æ±‚å®Œæˆ
    buffer_time = round_time * GLOBAL_CONFIG.get('buffer_ratio', 0.5)
    # ç¡®ä¿ç¼“å†²æ—¶é—´ä¸è¶…è¿‡round_timeçš„50%
    buffer_time = min(buffer_time, round_time * 0.5)
    # å®é™…å¯ç”¨çš„å‘é€æ—¶é—´çª—å£
    effective_round_time = round_time - buffer_time

    # å°†æ¯åˆ†é’Ÿè¯·æ±‚æ•°è½¬æ¢ä¸ºæ¯ç§’è¯·æ±‚æ•°
    rate_per_second = rate_lambda / 60.0

    if rate_per_second <= 0:
        rate_per_second = 0.001

    # åŸºç¡€æ—¶é—´é—´éš”
    base_interval = 1 / rate_per_second

    # ä¼°ç®—æ€»è¯·æ±‚æ•°ï¼ŒåŸºäºå®Œæ•´çš„round_timeï¼Œè€Œä¸æ˜¯effective_round_time
    # è¿™æ ·å³ä½¿æœ‰buffer timeï¼Œæˆ‘ä»¬ä»ç„¶ä¼šå‘é€å®Œæ•´çš„è¯·æ±‚æ•°é‡
    estimated_requests = int(round_time * rate_per_second)

    # ç”Ÿæˆæ‰€æœ‰è¯·æ±‚çš„æ—¶é—´ç‚¹
    request_times = []
    global_start_time = time.time()  # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå…¨å±€å¼€å§‹æ—¶é—´

    # ä»0å¼€å§‹ï¼Œæ²¡æœ‰éšæœºåç§»
    start_offset = 0

    # å…ˆç”ŸæˆåŸºç¡€æ—¶é—´ç‚¹ï¼ˆç›¸å¯¹äºå¼€å§‹æ—¶é—´çš„åç§»ï¼‰
    base_times = []
    current_offset = start_offset  # ä»åç§»å¼€å§‹

    # è®¡ç®—æ—¶é—´å‹ç¼©æ¯”ä¾‹ï¼Œå°†round_timeçš„è¯·æ±‚å‹ç¼©åˆ°effective_round_timeå†…
    compression_ratio = effective_round_time / round_time if round_time > 0 else 1.0

    for i in range(estimated_requests):
        # æ ¹æ®åˆ†å¸ƒç±»å‹è®¡ç®—é—´éš”
        if distribution.lower() == "poisson":
            # æ³Šæ¾åˆ†å¸ƒ
            interval = float(np.random.exponential(base_interval))
        elif distribution.lower() == "normal":
            # æ­£æ€åˆ†å¸ƒï¼Œä½¿ç”¨å›ºå®šæ ‡å‡†å·®
            std_dev = base_interval * 0.3  # å›ºå®šæ ‡å‡†å·®ä¸ºé—´éš”çš„30%
            interval = base_interval + float(np.random.normal(0, std_dev))
            interval = max(0.001, interval)  # ç¡®ä¿é—´éš”ä¸ºæ­£
        else:
            # å‡åŒ€åˆ†å¸ƒ
            interval = base_interval  # ä½¿ç”¨å›ºå®šé—´éš”

        # åº”ç”¨å‹ç¼©æ¯”ä¾‹ï¼Œå°†è¯·æ±‚é—´éš”å‹ç¼©
        compressed_interval = interval * compression_ratio

        current_offset += compressed_interval
        if current_offset > effective_round_time:  # ç¡®ä¿ä¸è¶…å‡ºæœ‰æ•ˆæ—¶é—´çª—å£
            break
        base_times.append(current_offset)

    for base_time in base_times:
        request_time = global_start_time + base_time
        request_times.append(request_time)

    # è®°å½•ç”Ÿæˆçš„è¯·æ±‚æ•°é‡
    experiment.logger.info(
        f"Generated {len(request_times)} requests for QPM {qmp_per_worker} in {effective_round_time:.1f}s effective window (buffer: {buffer_time:.1f}s)")

    # ä¿å­˜è¯·æ±‚æ—¶é—´åˆ—è¡¨åˆ°æ–‡ä»¶
    save_request_times_to_file(experiment, request_times, qmp_per_worker, global_start_time)

    return request_times


def save_request_times_to_file(experiment, request_times, qmp_per_worker, global_start_time):
    """å°†è¯·æ±‚æ—¶é—´åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼ŒæŒ‰å®¢æˆ·ç«¯åˆ†ç»„ï¼Œç´¯ç§¯è®°å½•æ¨¡å¼"""
    import os
    import json
    from datetime import datetime
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('tmp_result', exist_ok=True)
    
    # è·å–æ–‡ä»¶å
    timestamp = GLOBAL_CONFIG.get("monitor_file_time", datetime.now().strftime("%m_%d_%H_%M"))
    filename = f'tmp_result/request_times_{timestamp}.json'
    
    # è·å–å®¢æˆ·ç«¯ä¿¡æ¯
    client_id = getattr(experiment.client, 'client_id', 'unknown_client')
    client_type = getattr(experiment.client, 'client_type', 'unknown')
    worker_id = getattr(experiment, 'worker_id', 'unknown_worker')
    
    # è·å–è½®æ¬¡ä¿¡æ¯
    round_num = getattr(experiment, 'config_round', 0)
    
    # è½¬æ¢æ—¶é—´æˆ³ä¸ºç›¸å¯¹æ—¶é—´ï¼ˆä¾¿äºåˆ†æï¼‰
    relative_times = [t - global_start_time for t in request_times]
    
    # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
    client_round_data = {
        'client_id': client_id,
        'client_type': client_type,
        'worker_id': worker_id,
        'round_num': round_num,
        'qmp_per_worker': qmp_per_worker,
        'round_time': experiment.round_time,
        'distribution': experiment.distribution,
        'time_ratio': experiment.time_ratio,
        'global_start_time': global_start_time,
        'global_start_time_24h': datetime.fromtimestamp(global_start_time).strftime("%Y-%m-%d %H:%M:%S"),
        'total_requests': len(request_times),
        'request_times': {
            'absolute_timestamps': request_times,  # ç»å¯¹æ—¶é—´æˆ³
            'relative_seconds': relative_times,    # ç›¸å¯¹äºå¼€å§‹æ—¶é—´çš„ç§’æ•°
            'first_request_at': relative_times[0] if relative_times else 0,
            'last_request_at': relative_times[-1] if relative_times else 0,
            'time_span': relative_times[-1] - relative_times[0] if len(relative_times) > 1 else 0
        },
        'statistics': {
            'avg_interval': sum(relative_times[i+1] - relative_times[i] for i in range(len(relative_times)-1)) / max(len(relative_times)-1, 1) if len(relative_times) > 1 else 0,
            'min_interval': min(relative_times[i+1] - relative_times[i] for i in range(len(relative_times)-1)) if len(relative_times) > 1 else 0,
            'max_interval': max(relative_times[i+1] - relative_times[i] for i in range(len(relative_times)-1)) if len(relative_times) > 1 else 0
        },
        'recorded_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # è¯»å–ç°æœ‰æ–‡ä»¶æˆ–åˆ›å»ºæ–°æ–‡ä»¶
    all_data = {}
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            all_data = {}
    
    # å¦‚æœæ–‡ä»¶æ˜¯æ–°çš„ï¼Œåˆå§‹åŒ–ç»“æ„
    if 'metadata' not in all_data:
        all_data['metadata'] = {
            'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'experiment_type': getattr(experiment, 'exp_type', 'unknown'),
            'total_clients': 0,
            'total_rounds': 0
        }
        all_data['experiments'] = {}
    
    # åˆ›å»ºå”¯ä¸€çš„å®éªŒé”®ï¼šclient_id + round_num
    experiment_key = f"{client_id}_round_{round_num}"
    
    # æ·»åŠ å®éªŒæ•°æ®
    all_data['experiments'][experiment_key] = client_round_data
    
    # æ›´æ–°å…ƒæ•°æ®
    unique_clients = set()
    unique_rounds = set()
    for exp_key, exp_data in all_data['experiments'].items():
        unique_clients.add(exp_data['client_id'])
        unique_rounds.add(exp_data['round_num'])
    
    all_data['metadata']['total_clients'] = len(unique_clients)
    all_data['metadata']['total_rounds'] = len(unique_rounds)
    all_data['metadata']['total_experiments'] = len(all_data['experiments'])
    all_data['metadata']['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, indent=2, ensure_ascii=False)
        
        experiment.logger.info(f"Request times saved to {filename} for {experiment_key}")
        experiment.logger.info(f"  - Total requests: {len(request_times)}")
        experiment.logger.info(f"  - Time span: {relative_times[-1] - relative_times[0]:.2f}s" if len(relative_times) > 1 else "  - Single request")
        experiment.logger.info(f"  - Average interval: {client_round_data['statistics']['avg_interval']:.3f}s")
        experiment.logger.info(f"  - File now contains {len(all_data['experiments'])} experiments from {len(unique_clients)} clients")
        
    except Exception as e:
        experiment.logger.error(f"Failed to save request times to {filename}: {e}")


async def worker(experiment, selected_clients, semaphore, results, worker_id, worker_json, qmp_per_worker):
    """æ¯ä¸ªtaskå‘é€å•ä¸ªè¯·æ±‚ï¼Œä½¿ç”¨é¢„å…ˆè®¡ç®—çš„æ—¶é—´ç‚¹æ§åˆ¶é—´éš”"""
    assert worker_json is not None, "sample_content is None!"
    assert isinstance(worker_json, list), f"sample_content is not a list! type={type(worker_json)}"
    assert len(worker_json) > 0, "sample_content is empty!"

    global_start_time = time.time()
    request_count = 0
    drift_time = 0
    completed = 0
    tasks = []

    # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„tokenè®¡æ•°å™¨
    tokens_counter = ThreadSafeCounter()

    # è·å–å®¢æˆ·ç«¯IDç”¨äºæ—¥å¿—
    client_id = getattr(experiment.client, 'client_id', f'unknown_client_worker_{worker_id}')

    # é¢„å…ˆè®¡ç®—æ‰€æœ‰è¯·æ±‚çš„æ—¶é—´ç‚¹
    request_times = calculate_all_request_times(experiment, qmp_per_worker)

    for target_time in request_times:
        if time.time() - global_start_time >= experiment.round_time:
            break
        current_time = time.time()
        if target_time <= current_time:
            # å¦‚æœç›®æ ‡æ—¶é—´å·²è¿‡ï¼Œç›´æ¥å‘é€è¯·æ±‚
            drift_time = current_time - target_time
        else:
            # å¦‚æœè¿˜æ²¡åˆ°ç›®æ ‡æ—¶é—´ï¼Œå…ˆsleep
            sleep_time = target_time - current_time
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
            else:
                experiment.logger.warning(
                    f"Client {client_id}: Warning: Negative sleep time detected: {sleep_time:.6f} seconds")
                continue

        # å‘é€è¯·æ±‚ï¼ˆä¸ç®¡æ˜¯å¦éœ€è¦sleepï¼Œéƒ½ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼‰
        request = random.choice(worker_json)
        selected_client = selected_clients[worker_id % len(selected_clients)]

        # ç”Ÿæˆrequest_idå¹¶åœ¨åˆ›å»ºtaskæ—¶å°±é›†æˆ
        request_id = generate_unique_request_id(client_id, f"worker_{worker_id}")

        task = asyncio.create_task(
            process_request(selected_client, experiment, request, worker_id, results, semaphore, tokens_counter,
                            request_id)
        )

        tasks.append(task)
        request_count += 1

    elapsed = time.time() - global_start_time
    remaining_time = experiment.round_time - elapsed
    if remaining_time > experiment.round_time * GLOBAL_CONFIG.get('buffer_ratio', 0.5) * 1.1:
        if remaining_time > (experiment.round_time * GLOBAL_CONFIG.get('buffer_ratio', 0.5) * 2):
            experiment.logger.warning(
                f"Client {client_id}: Warning: Not enough requests to fill the round time. Sleeping for {remaining_time:.2f} seconds")
        await asyncio.sleep(remaining_time)
    else:
        experiment.logger.info(f"Client {client_id}: reached the end of the round time.")

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    if tasks:
        # è®¡ç®—æ€»è€—æ—¶
        total_elapsed_time = time.time() - global_start_time

        completed = sum(1 for task in tasks if task.done())
        cancelled_count = sum(1 for task in tasks if task.cancelled())

        experiment.logger.info(
            f"Client {client_id} Worker {worker_id}: Total tasks: {request_count}, Completed: {completed}, Cancelled: {cancelled_count}")
        experiment.logger.info(
            f"Client {client_id} Worker {worker_id}: Task completion rate: {completed / len(tasks) * 100:.2f}%")
        experiment.logger.info(f"Client {client_id} Worker {worker_id}: Total tokens processed: {tokens_counter.value}")
        experiment.logger.info(
            f"Client {client_id} Worker {worker_id}: Total elapsed time: {total_elapsed_time:.2f} seconds, Round time: {experiment.round_time:.2f} seconds, More than round time: {total_elapsed_time - experiment.round_time:.2f} seconds")

        for task in tasks:
            task.cancel()

    return completed, drift_time, request_count


async def process_request(openai, experiment, request, worker_id, results, semaphore, tokens_counter, request_id=None):
    # å¦‚æœæ²¡æœ‰æä¾›request_idï¼Œç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„
    if request_id is None:
        client_id = getattr(experiment.client, 'client_id', f'unknown_client_worker_{worker_id}')
        request_id = generate_unique_request_id(client_id, f"worker_{worker_id}")

    async with semaphore:
        try:
            result = await make_request(openai, experiment, request, request_id=request_id)
            if result:
                output_tokens = result[0]  # ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯output_tokens
                # åŸå­æ€§åœ°æ›´æ–°tokenè®¡æ•°
                new_total = tokens_counter.add(output_tokens)
                results.append(result)

        except Exception as e:
            logging.error(
                f"Worker {worker_id} {experiment.config_round + 1} round for client {experiment.client_index} raised an exception: {e}")
